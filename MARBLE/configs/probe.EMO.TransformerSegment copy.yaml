# =============================================================================
# 1. EMO Task (Emotion Regression) - probe.EMO.TransformerSegment.yaml
# =============================================================================
seed_everything: 1234
ckpt_path: null # to resume

trainer:
  fast_dev_run: false
  accelerator: gpu
  devices: [0]
  accumulate_grad_batches: 4
  num_nodes: 1
  precision: bf16
  max_epochs: 50
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 10
  log_every_n_steps: 5

  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: "./output/probe.EMO.TransformerSegment/checkpoints/"
        filename: "best"
        save_top_k: 1
    - class_path: marble.modules.callbacks.LoadLatestCheckpointCallback
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: lightning.pytorch.callbacks.early_stopping.EarlyStopping
      init_args:
        monitor: "val/r2"
        patience: 20
        mode: "max"
    - class_path: marble.encoders.transformer_segment_encoder.TransformerStatsCallback  # 통계 콜백 추가
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: "marble"
      name: "probe.EMO.TransformerSegment"
      save_dir: "./output/probe.EMO.TransformerSegment/"

model:
  class_path: marble.tasks.EMO.probe.ProbeAudioTask
  init_args:
    sample_rate: 24000
    use_ema: false

    encoder:
      class_path: marble.encoders.transformer_segment_encoder.TransformerSegmentEncoder
      init_args:
        muq_model_name: "OpenMuQ/MuQ-large-msd-iter"
        transformer_checkpoint_path: "/home/jay/MusicAI/transformer_60sec_mlm.pth"
        target_sr: 24000
        muq_segment_seconds: 10  # MuQ 처리용 10초 세그먼트
        min_samples_threshold: 2048
        emb_dim: 1024
        heads: 8
        hidden: 1024
        layers: 4
        max_len: 100000
        dropout: 0.1
      
    emb_transforms: []  # No additional transforms needed
    
    decoders:
      - class_path: marble.modules.decoders.MLPDecoder
        init_args:
          in_dim: 1024  # Transformer output dimension
          out_dim: 2   # arousal + valence
          hidden_layers: [512]
          activation_fn: 
            class_path: torch.nn.ReLU
          dropout: 0.2

    losses:
      - class_path: torch.nn.MSELoss
        init_args:
          reduction: mean

    metrics:
      train:
        r2:
          class_path: torchmetrics.R2Score
          init_args:
            multioutput: uniform_average
        arousal_r2:
          class_path: marble.tasks.EMO.probe.SliceR2
          init_args:
            dim: 0
        valence_r2:
          class_path: marble.tasks.EMO.probe.SliceR2
          init_args:
            dim: 1
      val:
        r2:
          class_path: torchmetrics.R2Score
          init_args:
            multioutput: uniform_average
        arousal_r2:
          class_path: marble.tasks.EMO.probe.SliceR2
          init_args:
            dim: 0
        valence_r2:
          class_path: marble.tasks.EMO.probe.SliceR2
          init_args:
            dim: 1
      test:
        r2:
          class_path: torchmetrics.R2Score
          init_args:
            multioutput: uniform_average
        arousal_r2:
          class_path: marble.tasks.EMO.probe.SliceR2
          init_args:
            dim: 0
        valence_r2:
          class_path: marble.tasks.EMO.probe.SliceR2
          init_args:
            dim: 1
    
data:
  class_path: marble.tasks.EMO.datamodule.EMODataModule
  init_args:
    batch_size: 4  # 더 작은 배치 크기 (메모리 절약)
    num_workers: 4

    train:
      class_path: marble.tasks.EMO.datamodule.EMOAudioTrain
      init_args:
        sample_rate: 24000
        channels: 1
        clip_seconds: 15
        min_clip_ratio: 0.8
        channel_mode: first
        jsonl: data/EMO/EMO.train.jsonl
    val:
      class_path: marble.tasks.EMO.datamodule.EMOAudioVal
      init_args:
        sample_rate: 24000
        channels: 1
        clip_seconds: 15
        min_clip_ratio: 0.8
        channel_mode: first
        jsonl: data/EMO/EMO.val.jsonl
    test:
      class_path: marble.tasks.EMO.datamodule.EMOAudioTest
      init_args:
        sample_rate: 24000
        channels: 1
        clip_seconds: 15
        min_clip_ratio: 0.8
        channel_mode: first
        jsonl: data/EMO/EMO.test.jsonl

optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 1e-3

lr_scheduler:
  class_path: lightning.pytorch.cli.ReduceLROnPlateau
  init_args:
    mode: "max"
    factor: 0.5
    patience: 5
    monitor: "val/r2"

---

# =============================================================================
# 2. GS Task (Key Detection) - probe.GS.TransformerSegment.yaml
# =============================================================================
seed_everything: 1234
ckpt_path: null

trainer:
  fast_dev_run: false
  accelerator: gpu
  devices: [0]
  accumulate_grad_batches: 4
  num_nodes: 1
  precision: bf16
  max_epochs: 50
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 10
  log_every_n_steps: 5

  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: "./output/probe.GS.TransformerSegment/checkpoints/"
        filename: "best"
        save_top_k: 1
    - class_path: marble.modules.callbacks.LoadLatestCheckpointCallback
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: lightning.pytorch.callbacks.early_stopping.EarlyStopping
      init_args:
        monitor: "val/weighted_score"
        patience: 20
        mode: "max"
    - class_path: marble.encoders.transformer_segment_encoder.TransformerStatsCallback
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: "marble"
      name: "probe.GS.TransformerSegment"
      save_dir: "./output/probe.GS.TransformerSegment/"

model:
  class_path: marble.tasks.GS.probe.ProbeAudioTask
  init_args:
    sample_rate: 24000
    use_ema: false

    encoder:
      class_path: marble.encoders.transformer_segment_encoder.TransformerSegmentEncoder
      init_args:
        muq_model_name: "OpenMuQ/MuQ-large-msd-iter"
        transformer_checkpoint_path: "/home/jay/MusicAI/transformer_60sec_mlm.pth"
        target_sr: 24000
        segment_seconds: 60
        min_samples_threshold: 2048
        emb_dim: 1024
        heads: 8
        hidden: 1024
        layers: 4
        max_len: 100000
        dropout: 0.1
      
    emb_transforms: []
    
    decoders:
      - class_path: marble.modules.decoders.MLPDecoder
        init_args:
          in_dim: 1024
          out_dim: 24  # 24 keys
          hidden_layers: [512]
          activation_fn: 
            class_path: torch.nn.ReLU
          dropout: 0.2

    losses:
      - class_path: torch.nn.CrossEntropyLoss
        init_args:
          reduction: mean

    metrics:
      train:
        acc:
          class_path: torchmetrics.Accuracy
          init_args:
            num_classes: 24
            task: multiclass
        weighted_score:
          class_path: marble.tasks.GS.probe.KeyWeightedScore
      val:
        acc:
          class_path: torchmetrics.Accuracy
          init_args:
            num_classes: 24
            task: multiclass
        weighted_score:
          class_path: marble.tasks.GS.probe.KeyWeightedScore
      test:
        acc:
          class_path: torchmetrics.Accuracy
          init_args:
            num_classes: 24
            task: multiclass
        weighted_score:
          class_path: marble.tasks.GS.probe.KeyWeightedScore
    
data:
  class_path: marble.tasks.GS.datamodule.GSDataModule
  init_args:
    batch_size: 4
    num_workers: 4

    train:
      class_path: marble.tasks.GS.datamodule.GSAudioTrain
      init_args:
        sample_rate: 24000
        channels: 1
        clip_seconds: 30
        min_clip_ratio: 0.8
        channel_mode: first
        jsonl: data/GS/GS.train.jsonl
    val:
      class_path: marble.tasks.GS.datamodule.GSAudioVal
      init_args:
        sample_rate: 24000
        channels: 1
        clip_seconds: 30
        min_clip_ratio: 0.8
        channel_mode: first
        jsonl: data/GS/GS.val.jsonl
    test:
      class_path: marble.tasks.GS.datamodule.GSAudioTest
      init_args:
        sample_rate: 24000
        channels: 1
        clip_seconds: 30
        min_clip_ratio: 0.8
        channel_mode: first
        jsonl: data/GS/GS.test.jsonl

optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 1e-3

lr_scheduler:
  class_path: lightning.pytorch.cli.ReduceLROnPlateau
  init_args:
    mode: "max"
    factor: 0.1
    patience: 5
    monitor: "val/weighted_score"

---

# =============================================================================
# 3. GTZANGenre Task (Genre Classification) - probe.GTZANGenre.TransformerSegment.yaml
# =============================================================================
seed_everything: 1234
ckpt_path: null

trainer:
  fast_dev_run: false
  accelerator: gpu
  devices: [0]
  accumulate_grad_batches: 4
  num_nodes: 1
  precision: bf16
  max_epochs: 50
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 10
  log_every_n_steps: 5

  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: "./output/probe.GTZANGenre.TransformerSegment/checkpoints/"
        filename: "best"
        save_top_k: 1
    - class_path: marble.modules.callbacks.LoadLatestCheckpointCallback
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: lightning.pytorch.callbacks.early_stopping.EarlyStopping
      init_args:
        monitor: "val/acc"
        patience: 20
        mode: "max"
    - class_path: marble.encoders.transformer_segment_encoder.TransformerStatsCallback
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: "marble"
      name: "probe.GTZANGenre.TransformerSegment"
      save_dir: "./output/probe.GTZANGenre.TransformerSegment/"

model:
  class_path: marble.tasks.GTZANGenre.probe.ProbeAudioTask
  init_args:
    sample_rate: 24000
    use_ema: false

    encoder:
      class_path: marble.encoders.transformer_segment_encoder.TransformerSegmentEncoder
      init_args:
        muq_model_name: "OpenMuQ/MuQ-large-msd-iter"
        transformer_checkpoint_path: "/home/jay/MusicAI/transformer_60sec_mlm.pth"
        target_sr: 24000
        segment_seconds: 60
        min_samples_threshold: 2048
        emb_dim: 1024
        heads: 8
        hidden: 1024
        layers: 4
        max_len: 100000
        dropout: 0.1
      
    emb_transforms: []
    
    decoders:
      - class_path: marble.modules.decoders.MLPDecoder
        init_args:
          in_dim: 1024
          out_dim: 10  # 10 genres
          hidden_layers: [512]
          activation_fn: 
            class_path: torch.nn.ReLU
          dropout: 0.2

    losses:
      - class_path: torch.nn.CrossEntropyLoss
        init_args:
          reduction: mean

    metrics:
      train:
        acc:
          class_path: torchmetrics.Accuracy
          init_args:
            num_classes: 10
            task: multiclass
      val:
        acc:
          class_path: torchmetrics.Accuracy
          init_args:
            num_classes: 10
            task: multiclass
      test:
        acc:
          class_path: torchmetrics.Accuracy
          init_args:
            num_classes: 10
            task: multiclass
    
data:
  class_path: marble.tasks.GTZANGenre.datamodule.GTZANGenreDataModule
  init_args:
    batch_size: 4
    num_workers: 4

    train:
      class_path: marble.tasks.GTZANGenre.datamodule.GTZANGenreAudioTrain
      init_args:
        sample_rate: 24000
        channels: 1
        clip_seconds: 30
        min_clip_ratio: 0.8
        channel_mode: first
        jsonl: data/GTZANGenre/GTZANGenre.train.jsonl
    val:
      class_path: marble.tasks.GTZANGenre.datamodule.GTZANGenreAudioVal
      init_args:
        sample_rate: 24000
        channels: 1
        clip_seconds: 30
        min_clip_ratio: 0.8
        channel_mode: first
        jsonl: data/GTZANGenre/GTZANGenre.val.jsonl
    test:
      class_path: marble.tasks.GTZANGenre.datamodule.GTZANGenreAudioTest
      init_args:
        sample_rate: 24000
        channels: 1
        clip_seconds: 30
        min_clip_ratio: 0.8
        channel_mode: first
        jsonl: data/GTZANGenre/GTZANGenre.test.jsonl

optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 1e-3

lr_scheduler:
  class_path: lightning.pytorch.cli.ReduceLROnPlateau
  init_args:
    mode: "max"
    factor: 0.5
    patience: 5
    monitor: "val/acc"

---

# =============================================================================
# 4. MTGGenre Task (Multi-label Genre Tagging) - probe.MTGGenre.TransformerSegment.yaml
# =============================================================================
seed_everything: 1234
ckpt_path: null

trainer:
  fast_dev_run: false
  accelerator: gpu
  devices: [0]
  accumulate_grad_batches: 8  # 더 큰 accumulation (메모리 절약)
  num_nodes: 1
  precision: bf16
  max_epochs: 50
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 10
  log_every_n_steps: 5

  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: "./output/probe.MTGGenre.TransformerSegment/checkpoints/"
        filename: "best"
        save_top_k: 1
    - class_path: marble.modules.callbacks.LoadLatestCheckpointCallback
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: lightning.pytorch.callbacks.early_stopping.EarlyStopping
      init_args:
        monitor: "val/roc_auc"
        patience: 20
        mode: "max"
    - class_path: marble.encoders.transformer_segment_encoder.TransformerStatsCallback
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: "marble"
      name: "probe.MTGGenre.TransformerSegment"
      save_dir: "./output/probe.MTGGenre.TransformerSegment/"

model:
  class_path: marble.tasks.MTGGenre.probe.ProbeAudioTask
  init_args:
    sample_rate: 24000
    use_ema: false

    encoder:
      class_path: marble.encoders.transformer_segment_encoder.TransformerSegmentEncoder
      init_args:
        muq_model_name: "OpenMuQ/MuQ-large-msd-iter"
        transformer_checkpoint_path: "/home/jay/MusicAI/transformer_60sec_mlm.pth"
        target_sr: 24000
        segment_seconds: 60
        min_samples_threshold: 2048
        emb_dim: 1024
        heads: 8
        hidden: 1024
        layers: 4
        max_len: 100000
        dropout: 0.1
      
    emb_transforms: []
    
    decoders:
      - class_path: marble.modules.decoders.MLPDecoder
        init_args:
          in_dim: 1024
          out_dim: 87  # Number of genre tags in MTG-Jamendo
          hidden_layers: [512]
          activation_fn: 
            class_path: torch.nn.ReLU
          dropout: 0.2

    losses:
      - class_path: torch.nn.BCEWithLogitsLoss  # Multi-label classification
        init_args:
          reduction: mean

    metrics:
      train:
        roc_auc:
          class_path: torchmetrics.AUROC
          init_args:
            task: multilabel
            num_labels: 87
      val:
        roc_auc:
          class_path: torchmetrics.AUROC
          init_args:
            task: multilabel
            num_labels: 87
      test:
        roc_auc:
          class_path: torchmetrics.AUROC
          init_args:
            task: multilabel
            num_labels: 87
    
data:
  class_path: marble.tasks.MTGGenre.datamodule.MTGGenreDataModule
  init_args:
    batch_size: 2  # 매우 작은 배치 크기 (메모리 절약)
    num_workers: 4

    train:
      class_path: marble.tasks.MTGGenre.datamodule.MTGGenreAudioTrain
      init_args:
        sample_rate: 24000
        channels: 1
        clip_seconds: 30
        min_clip_ratio: 0.8
        channel_mode: first
        jsonl: data/MTGGenre/MTGGenre.train.jsonl
    val:
      class_path: marble.tasks.MTGGenre.datamodule.MTGGenreAudioVal
      init_args:
        sample_rate: 24000
        channels: 1
        clip_seconds: 30
        min_clip_ratio: 0.8
        channel_mode: first
        jsonl: data/MTGGenre/MTGGenre.val.jsonl
    test:
      class_path: marble.tasks.MTGGenre.datamodule.MTGGenreAudioTest
      init_args:
        sample_rate: 24000
        channels: 1
        clip_seconds: 30
        min_clip_ratio: 0.8
        channel_mode: first
        jsonl: data/MTGGenre/MTGGenre.test.jsonl

optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 5e-4  # 더 작은 학습률

lr_scheduler:
  class_path: lightning.pytorch.cli.ReduceLROnPlateau
  init_args:
    mode: "max"
    factor: 0.5
    patience: 5
    monitor: "val/roc_auc"